{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Change Detection Results \n",
    "\n",
    "This is an intro notebook on using geospatial data packages like folium and H3 to aggregate and visualize polygon  from the Planet change detection analytic feed using hierarchical indices. H3 is Uber's spatial indexing system using a hexagonal grid system optimized for visualizing geospatial data. You can read more here (https://eng.uber.com/h3/). Folium provides an intuitive python library for creating layered maps on leaflet. You can learn more here (https://www.coursera.org/lecture/python-for-data-visualization/introduction-to-folium-CpjW0). You can learn more about Planet's analytic feeds here (https://www.planet.com/products/analytics/) and request access to the building change detection feed here (https://learn.planet.com/explore-product-announcement.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from h3 import h3\n",
    "from pprint import pprint\n",
    "from IPython.display import IFrame\n",
    "import folium\n",
    "import webbrowser\n",
    "from folium import Map\n",
    "import geopandas as gpd\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Defining some helper functions to get define h3 indices from rows with geometriesparse subscription results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4,
     7
    ]
   },
   "outputs": [],
   "source": [
    "def lat_lng_to_h3(row):\n",
    "    return h3.geo_to_h3(row['lat'], row['lng'], H3_LEVEL)\n",
    "\n",
    "def get_next_link(results_json):\n",
    "    \"\"\"Given a response json from one page of subscription results, get the url for the next page of results.\"\"\"\n",
    "    for link in results_json['links']:\n",
    "        if link['rel'] == 'next':\n",
    "            return link['href']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data access and preparation\n",
    "\n",
    "Paginating through a change detection subscriptions, accessing results and capturing those results into a geopandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "LIVE_BASE_URL = 'https://api.planet.com/analytics/'\n",
    "# if your Planet API Key is not set as an environment variable, you can paste it below\n",
    "PL_API_KEY = os.environ['PL_API_KEY']\n",
    "# construct auth tuple for use in the requests library\n",
    "BASIC_AUTH = (PL_API_KEY, '')\n",
    "EXISTING_GEOJSON = True\n",
    "H3_LEVEL = 4\n",
    "\n",
    "'''\n",
    "bbox = left,bottom,right,top\n",
    "bbox = min Longitude , min Latitude , max Longitude , max Latitude\n",
    "'''\n",
    "# Noting Subscription IDs and associated bounding boxes we need to filter API search\n",
    "SUB_ID_CA = '0163608e-0afb-477e-a500-3a87819351a1'\n",
    "BBOX_LONGBEACH = '-118.24896793,33.710663987,-118.06315999,33.8854059'\n",
    "BBOX_RIVERSIDE = '-117.79347778,33.62802278,-116.90089412,34.2035886'\n",
    "\n",
    "SUBSCRIPTION_ID = SUB_ID_CA\n",
    "BBOX = BBOX_RIVERSIDE\n",
    "\n",
    "if EXISTING_GEOJSON:\n",
    "    gdf = gpd.read_file('./data/collection_0163608e-0afb-477e-a500-3a87819351a1.geojson')\n",
    "else:\n",
    "    # results_url = LIVE_BASE_URL + 'collections/' + SUBSCRIPTION_ID + '/items?bbox=' + BBOX\n",
    "    results_url = LIVE_BASE_URL + 'collections/' + SUBSCRIPTION_ID + '/items'\n",
    "    # Get subscription results collection\n",
    "    resp = requests.get(results_url, auth=BASIC_AUTH)\n",
    "    #checking access\n",
    "    if resp.status_code == 200:\n",
    "        print('Yay, you can access analytic feed results!')\n",
    "        subscription_results = resp.json()\n",
    "    else:\n",
    "        print('Something is wrong:', resp.content)\n",
    "\n",
    "    latest_feature = subscription_results['features'][0]\n",
    "    creation_datestring = latest_feature['created']\n",
    "    print('latest feature creation date:', creation_datestring)\n",
    "\n",
    "    feature_collection = {'type': 'FeatureCollection', 'features': []}\n",
    "    next_link = results_url\n",
    "\n",
    "    bufffer_count =0\n",
    "\n",
    "    while next_link:\n",
    "        bufffer_count += 1\n",
    "        print('result collection : ', bufffer_count)\n",
    "        results = requests.get(next_link, auth=BASIC_AUTH).json()\n",
    "        next_features = results['features']\n",
    "        if next_features:\n",
    "            latest_feature_creation = parse(next_features[0]['created']).date()\n",
    "            earliest_feature_creation = parse(next_features[-1]['created']).date()\n",
    "            print('Fetched {} features fetched ({}, {})'.format(\n",
    "                len(next_features), earliest_feature_creation, latest_feature_creation))\n",
    "            feature_collection['features'].extend(next_features)\n",
    "            next_link = get_next_link(results)\n",
    "        else:\n",
    "            next_link = None\n",
    "\n",
    "    print('Total features: {}'.format(len(feature_collection['features'])))\n",
    "\n",
    "    filename = 'collection_{}.geojson'.format(SUBSCRIPTION_ID)\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(feature_collection, file)\n",
    "\n",
    "    gdf = gpd.read_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroid detection and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['lat'] = gdf.centroid.map(lambda p: p.y)\n",
    "gdf['lng'] = gdf.centroid.map(lambda p: p.x)\n",
    "gdf['h3'] = gdf.apply(lat_lng_to_h3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the data from Planet look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21743, 12)\n",
      "id                                   67093241-0c50-4f73-aacf-c75f4566a3d5\n",
      "label                                                            building\n",
      "object_area_m2                                                    773.879\n",
      "observed                                              2019-06-01T00:00:00\n",
      "source_last_acquired                                  2019-07-01T00:00:00\n",
      "source_mosaic                                                        None\n",
      "source_mosaic_name                          global_monthly_2019_06_mosaic\n",
      "source_quad_id                                                   340-1284\n",
      "geometry                POLYGON ((-120.177512152137 41.52711793718, -1...\n",
      "lat                                                                41.527\n",
      "lng                                                              -120.177\n",
      "h3                                                        842812bffffffff\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(gdf.shape)\n",
    "print(gdf.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Aggregate change polygons by h3 cells and derive a count of detected change assets. We define a few functions to hierarchical clustering of the results and create a color map based on the counts within in each hexagonal cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     6,
     22,
     55,
     89
    ]
   },
   "outputs": [],
   "source": [
    "# Clustering with H3 is easy. \n",
    "# Just iterate through all points and aggregate the hit counts for each key. \n",
    "# Here we don't care about noise, all points are good.\n",
    "def get_relevant_clusters(gdf):\n",
    "    #Empty dict\n",
    "    h3_clusters = dict()\n",
    "    for index, row in gdf.iterrows():\n",
    "        key = row['h3']\n",
    "        if key in h3_clusters:\n",
    "            h3_clusters[key][\"count\"] += 1\n",
    "        else:\n",
    "            h3_clusters[key] = {\"count\": 1,\n",
    "                                \"geom\": h3.h3_to_geo_boundary(h3_address=key)}\n",
    "\n",
    "    # relevant cluseters\n",
    "    relevant_clusters = { key:value for (key,value) in h3_clusters.items() if value['count'] >= 1}\n",
    "\n",
    "    return relevant_clusters\n",
    "\n",
    "def create_map(clusters):\n",
    "    # The create_map function creates and populates a folium map from the given \n",
    "    # clusters dictionary. Each value of the dictionary is also a dictionary with \n",
    "    # two recognizable keys: geom and count. The value associated with the geom key \n",
    "    # is the list of locations of the hexagon's vertices. The value associated with\n",
    "    # the count key contains the number of accidents counted within this shape.\n",
    "    # Create the map object\n",
    "    map1 = Map(tiles=\"cartodbpositron\", \n",
    "          attr= '© <a href=\"http://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors © <a href=\"http://cartodb.com/attributions#basemaps\">CartoDB</a>')\n",
    "\n",
    "    # Convert the clusters dictionary items to polygons and add them to the map\n",
    "    for cluster in clusters.values():\n",
    "        points = cluster['geom']\n",
    "        # points = [p[::-1] for p in points]\n",
    "        tooltip = \"{0} assets\".format(cluster['count'])\n",
    "        polygon = folium.vector_layers.Polygon(locations=points, tooltip=tooltip,\n",
    "                                               fill=True, \n",
    "                                               color='#ff0000', \n",
    "                                               fill_color='#ff0000', \n",
    "                                               fill_opacity=0.4, weight=3, opacity=0.4)\n",
    "        polygon.add_to(map1)\n",
    "\n",
    "    # Determine the map bounding box\n",
    "    max_lat = gdf.lat.max()\n",
    "    min_lat = gdf.lat.min()\n",
    "    max_lon = gdf.lng.max()\n",
    "    min_lon = gdf.lng.min()\n",
    "    \n",
    "    # Fit the map to the bounds\n",
    "    map1.fit_bounds([[min_lat, min_lon], [max_lat, max_lon]])\n",
    "    \n",
    "    return map1\n",
    "\n",
    "def create_gradient_map(clusters):\n",
    "    # Create the map object\n",
    "    map1 = Map(tiles=\"cartodbpositron\", \n",
    "          attr= '© <a href=\"http://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors © <a href=\"http://cartodb.com/attributions#basemaps\">CartoDB</a>')\n",
    "\n",
    "\n",
    "    max_count = max([item['count'] for item in clusters.values()])\n",
    "\n",
    "    # Convert the clusters dictionary items to polygons and add them to the map\n",
    "    for cluster in clusters.values():\n",
    "        points = cluster['geom']\n",
    "        if cluster['count'] > 20: #some minimum threshold to filter out low volume build up\n",
    "            color = '#' + hex(int((0.25+(cluster['count']/(1.34*max_count)))*255))[2:].zfill(2) + '0000'\n",
    "            # color = '#' + hex(int((cluster['count']/max_count)*255))[2:].zfill(2) + '0000'\n",
    "            # points = [p[::-1] for p in points]\n",
    "            tooltip = \"{0} assets\".format(cluster['count'])\n",
    "            polygon = folium.vector_layers.Polygon(locations=points, tooltip=tooltip,\n",
    "                                               fill=True, \n",
    "                                               color=color, \n",
    "                                               fill_color=color, \n",
    "                                               fill_opacity=0.8, weight=3, opacity=0.0)\n",
    "            polygon.add_to(map1)\n",
    "\n",
    "    # Determine the map bounding box\n",
    "    max_lat = gdf.lat.max()\n",
    "    min_lat = gdf.lat.min()\n",
    "    max_lon = gdf.lng.max()\n",
    "    min_lon = gdf.lng.min()\n",
    "    \n",
    "    # Fit the map to the bounds\n",
    "    map1.fit_bounds([[min_lat, min_lon], [max_lat, max_lon]])\n",
    "    \n",
    "    return map1\n",
    "\n",
    "def show_map(map1, file_name):\n",
    "    # The show_map function saves the HTML generated by the map into a file and then opens a new browser tab with its contents\n",
    "    map1.save(file_name)\n",
    "    wb = webbrowser.open('file://' + os.path.realpath(file_name), new=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the Clustering\n",
    "\n",
    "Creating additional columns for centroids of the change detection results and H3 index, clustering loading them to a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_clusters = get_relevant_clusters(gdf)\n",
    "h3_map = create_gradient_map(relevant_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing a Change Heatmap\n",
    "\n",
    "We store the results with the subscription ID and zoom level and load it my in an IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"600\"\n",
       "            src=\"./data/0163608e-0afb-477e-a500-3a87819351a1_change_map_h3_4.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc098ec1e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h3_map.save(\"./data/{0}_change_map_h3_{1}.html\".format(SUBSCRIPTION_ID, H3_LEVEL))\n",
    "IFrame(src=\"./data/{0}_change_map_h3_{1}.html\".format(SUBSCRIPTION_ID, H3_LEVEL), width=700, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
